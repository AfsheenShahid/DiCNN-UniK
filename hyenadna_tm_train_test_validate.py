# -*- coding: utf-8 -*-
"""HyenaDNA-TM train_test_validate.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/101H9hU4YmcCD9kL634ik0n-le7SKnb4x
"""

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
from transformers import Trainer
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score



try:

    # the predictions, true labels, and metrics.
    print("Making predictions on the test dataset...")
    # Use the tokenized test dataset for prediction
    predictions_output = trainer.predict(tokenized_test_dataset)

    # Extract the predicted class indices from the logits (raw outputs)
    # The argmax function finds the index of the highest value for each prediction.
    predictions = np.argmax(predictions_output.predictions, axis=1)

    # Extract the true labels.
    true_labels = predictions_output.label_ids

    # Compute and print metrics
    accuracy = accuracy_score(true_labels, predictions)
    f1 = f1_score(true_labels, predictions, average='weighted')
    precision = precision_score(true_labels, predictions, average='weighted', zero_division=0)
    recall = recall_score(true_labels, predictions, average='weighted', zero_division=0)

    # Check if predictions and true_labels exist and are not empty.
    if len(predictions) == 0 or len(true_labels) == 0:
        print("Predictions or true labels are empty. Please check your dataset.")
    else:

        cm = confusion_matrix(true_labels, predictions)

        # Get the class labels. Assuming your labels are 0-9.
        # Use the label_encoder to get the actual class names
        class_labels = label_encoder.classes_


        # Plotting the confusion matrix as a heatmap for better visualization.
        # This gives a clear visual of where the model is making errors.
        plt.figure(figsize=(10, 8))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)

        # Add titles and labels for clarity.
        plt.title('Confusion Matrix for HyenaDNA Model on Test Set')
        plt.ylabel('True Label')
        plt.xlabel('Predicted Label')

        # Display the plot.
        plt.show()

        # Print the confusion matrix in a text-based format as well.
        print("\nText-based Confusion Matrix:")
        print(cm)

except NameError:
    # This error occurs if 'trainer' or 'test_dataset' is not found.
    print("Error: The 'trainer' and/or 'test_dataset' variables were not found. Please ensure the model training and evaluation steps have been run successfully.")
except Exception as e:
    print(f"An unexpected error occurred: {e}")

# update code for better figures
import os
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import time
from itertools import cycle

from sklearn.metrics import (
    accuracy_score, f1_score, precision_score, recall_score,
    confusion_matrix, precision_recall_fscore_support,
    matthews_corrcoef, roc_auc_score, roc_curve, auc
)
from sklearn.preprocessing import label_binarize
from transformers import Trainer, TrainingArguments
from datasets import Dataset
from google.colab import drive

# --- Setup and Initialization (Your existing code) ---
# NOTE: Assume 'tokenizer', 'model', 'label_encoder', and 'drive.mount' are
# defined and executed before this block.

# Mount and prepare output directory in Google Drive
# drive.mount('/content/drive') # Assuming this is mounted elsewhere
output_dir = "/content/gdrive/MyDrive/Model_Evaluation_Results_paper_test_ext_final_1" # Corrected path to gdrive

# Check if Google Drive is mounted
if not os.path.exists('/content/gdrive/MyDrive'):
    print("Error: Google Drive is not mounted. Please run the cell that mounts Google Drive first.")
else:
    os.makedirs(output_dir, exist_ok=True)

    # External validation files
    ext_files = ['flavi_raw_test_70_90.csv', 'flavi_raw_test_50_70.csv', 'flavi_raw_test_20_50.csv']
    max_seq_len = 11565 #Must match training

    # --- Model Size Calculation ---
    # Calculate the number of trainable parameters
    n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"üìä Model Parameter Count (Trainable): {n_params:,}")

    # Tokenization
    def tokenize_function(examples):
        return tokenizer(
            examples['Sequence'],
            max_length=max_seq_len,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

    # Minimal training args for prediction
    dummy_training_args = TrainingArguments(
        output_dir='./temp_eval',
        report_to="none"
    )

    trainer_for_prediction = Trainer(
        model=model,
        args=dummy_training_args
    )

    # For summary collection
    all_summaries = []

    # Loop through all external files
    for file in ext_files:
        dataset_name = os.path.splitext(file)[0]
        print(f"\nüìÅ Processing dataset: {dataset_name}")
        file_path = f'/content/{file}'

        try:
            external_df = pd.read_csv(file_path)
        except FileNotFoundError:
            print(f"‚ùå File '{file}' not found. Skipping.")
            continue

        external_df['Sequence'] = external_df['Sequence'].str.upper()
        external_df['labels'] = label_encoder.transform(external_df['label'])

        external_dataset = Dataset.from_pandas(external_df[['Sequence', 'labels']].reset_index(drop=True))
        tokenized_dataset = external_dataset.map(tokenize_function, batched=True, remove_columns=['Sequence'])

        # --- Inference Time Measurement ---
        start_time = time.time()
        predictions_output = trainer_for_prediction.predict(tokenized_dataset)
        end_time = time.time()

        total_time = end_time - start_time
        total_samples = len(external_df)
        inference_time_per_sample = total_time / total_samples

        print(f"‚è±Ô∏è Total inference time: {total_time:.4f} seconds")
        print(f"‚è±Ô∏è Inference time per sample: {inference_time_per_sample*1000:.4f} ms")


        pred_classes = np.argmax(predictions_output.predictions, axis=1)
        true_classes = predictions_output.label_ids
        pred_proba = predictions_output.predictions

        # Compute metrics (Your existing code)
        acc = accuracy_score(true_classes, pred_classes)
        f1 = f1_score(true_classes, pred_classes, average='weighted')
        prec = precision_score(true_classes, pred_classes, average='weighted', zero_division=0)
        rec = recall_score(true_classes, pred_classes, average='weighted', zero_division=0)
        mcc = matthews_corrcoef(true_classes, pred_classes)

        print(f"‚úÖ Accuracy: {acc:.4f}")
        # ... (other print statements)

        # --- Confusion Matrix Plot with Larger Labels ---
        cm = confusion_matrix(true_classes, pred_classes)
        class_labels = label_encoder.classes_.tolist()

        plt.figure(figsize=(10, 8))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=class_labels, yticklabels=class_labels,
                    annot_kws={"size": 14}) # Annot text size
        plt.title(f'Confusion Matrix for {dataset_name}', fontsize=16)
        plt.ylabel('True Label', fontsize=14)
        plt.xlabel('Predicted Label', fontsize=14)

        # Adjust ticks size for better readability
        plt.xticks(fontsize=12)
        plt.yticks(fontsize=12)

        plt.tight_layout()

        # Save in .png, .eps, and .pdf formats
        for ext in ['.png', '.eps', '.pdf']:
            cm_path = os.path.join(output_dir, f'{dataset_name}_confusion_matrix{ext}')
            plt.savefig(cm_path)
        plt.close()

        # ... (Per-class Metrics code remains the same) ...

        # --- ROC-AUC Plot with Larger Labels and NO Legend ---
        try:
            y_true_bin = label_binarize(true_classes, classes=np.arange(len(class_labels)))
            n_classes = y_true_bin.shape[1]

            fpr, tpr, roc_auc = {}, {}, {}
            for i in range(n_classes):
                fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], pred_proba[:, i])
                roc_auc[i] = auc(fpr[i], tpr[i])

            # Micro and Macro AUC calculation (Your existing code)
            fpr["micro"], tpr["micro"], _ = roc_curve(y_true_bin.ravel(), pred_proba.ravel())
            roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])
            all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))
            mean_tpr = np.zeros_like(all_fpr)
            for i in range(n_classes):
                mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])
            mean_tpr /= n_classes
            fpr["macro"] = all_fpr
            tpr["macro"] = mean_tpr
            roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])

            # ROC Curve Plot
            plt.figure(figsize=(10, 8))
            colors = cycle(plt.cm.tab10.colors)

            # Plot all curves without saving them to a legend variable
            for i, color in zip(range(n_classes), colors):
                plt.plot(fpr[i], tpr[i], color=color, lw=2)

            plt.plot(fpr["micro"], tpr["micro"], color='deeppink', linestyle=':', linewidth=2)
            plt.plot(fpr["macro"], tpr["macro"], color='navy', linestyle='--', linewidth=2)

            plt.plot([0, 1], [0, 1], 'k--', lw=2)
            plt.xlim([-0.05, 1.05])
            plt.ylim([-0.05, 1.05])

            # Increased label sizes
            plt.xlabel('False Positive Rate', fontsize=14)
            plt.ylabel('True Positive Rate', fontsize=14)
            plt.title(f'ROC Curve for {dataset_name}', fontsize=16)

            # Ticks size
            plt.xticks(fontsize=14)
            plt.yticks(fontsize=14)

            # **Legend is removed as requested**

            plt.tight_layout()

            # Save in .png, .eps, and .pdf formats
            for ext in ['.png', '.eps', '.pdf']:
                roc_path = os.path.join(output_dir, f'{dataset_name}_roc_curve{ext}')
                plt.savefig(roc_path)

            plt.close()
            print(f"üìà Saved ROC curve to {os.path.join(output_dir, f'{dataset_name}_roc_curve.pdf')}")

            auc_macro = roc_auc['macro']
            auc_micro = roc_auc['micro']

        except Exception as e:
            print(f"‚ö†Ô∏è ROC-AUC Calculation failed: {e}")
            auc_macro = auc_micro = np.nan

        # Add to summary (including new metrics)
        all_summaries.append({
            'Dataset': dataset_name,
            'Accuracy': acc,
            'F1 (weighted)': f1,
            'Precision (weighted)': prec,
            'Recall (weighted)': rec,
            'MCC': mcc,
            'AUC Macro': auc_macro,
            'AUC Micro': auc_micro,
            'Model Parameters': f"{n_params:,}", # Added
            'Inference Time per Sample (ms)': inference_time_per_sample * 1000 # Added
        })

    # Save final summary
    summary_df = pd.DataFrame(all_summaries)
    summary_path = os.path.join(output_dir, 'all_datasets_summary.csv')
    summary_df.to_csv(summary_path, index=False)
    print(f"\n‚úÖ Saved all summaries to: {summary_path}")
